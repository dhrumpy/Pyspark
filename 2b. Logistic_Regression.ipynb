{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Consulting Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Binary Customer Churn\n",
    "\n",
    "A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but want you to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. Luckily they have some historical data, can you help them out? Create a classification algorithm that will help classify whether or not a customer churned. Then the company can test this against incoming data for future customers to predict which customers will churn and assign them an account manager.\n",
    "\n",
    "The data is saved as customer_churn.csv. Here are the fields and their definitions:\n",
    "\n",
    "    Name : Name of the latest contact at Company\n",
    "    Age: Customer Age\n",
    "    Total_Purchase: Total Ads Purchased\n",
    "    Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
    "    Years: Totaly Years as a customer\n",
    "    Num_sites: Number of websites that use the service.\n",
    "    Onboard_date: Date that the name of the latest contact was onboarded\n",
    "    Location: Client HQ Address\n",
    "    Company: Name of Client Company\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/dhrumpy/spark-3.0.0-bin-hadoop3.2')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('churn').getOrCreate()\n",
    "\n",
    "data = spark.read.csv('customer_churn.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+-----------------+------------------+-------------------+\n",
      "|summary|              Age|   Total_Purchase|            Years|         Num_Sites|              Churn|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+-------------------+\n",
      "|  count|              900|              900|              900|               900|                900|\n",
      "|   mean|41.81666666666667|10062.82403333334| 5.27315555555555| 8.587777777777777|0.16666666666666666|\n",
      "| stddev|6.127560416916251|2408.644531858096|1.274449013194616|1.7648355920350969| 0.3728852122772358|\n",
      "|    min|             22.0|            100.0|              1.0|               3.0|                  0|\n",
      "|    max|             65.0|         18026.01|             9.15|              14.0|                  1|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---------+-----+\n",
      "| Age|Total_Purchase|Years|Num_Sites|Churn|\n",
      "+----+--------------+-----+---------+-----+\n",
      "|42.0|       11066.8| 7.22|      8.0|    1|\n",
      "|41.0|      11916.22|  6.5|     11.0|    1|\n",
      "|38.0|      12884.75| 6.67|     12.0|    1|\n",
      "|42.0|       8010.76| 6.71|     10.0|    1|\n",
      "|37.0|       9191.58| 5.56|      9.0|    1|\n",
      "+----+--------------+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(['Age', 'Total_Purchase', 'Years', 'Num_Sites', 'Churn']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.groupBy('Company').count().show(5)\n",
    "data = data.select(['Age', 'Total_Purchase', 'Years', 'Num_Sites', 'Churn']).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO VECTOR INDEXER REQUIRED HERE AS NO CATEGORICAL VARIABLES PRESENT HERE!\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['Age', 'Total_Purchase', 'Years', 'Num_Sites'], outputCol='features')\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "log_reg_churn = LogisticRegression(featuresCol='features', labelCol='Churn')\n",
    "pipeline = Pipeline(stages=[assembler,log_reg_churn]) # last entry is the actual model\n",
    "train_churn_data, test_churn_data = data.randomSplit([0.7,0.3])\n",
    "fit_model = pipeline.fit(train_churn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Churn|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = fit_model.transform(test_churn_data)\n",
    "results.select('Churn','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7520661157024794"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Churn')\n",
    "AUC = my_eval.evaluate(results)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---------+-----+--------------------+\n",
      "| Age|Total_Purchase|Years|Num_Sites|Churn|            features|\n",
      "+----+--------------+-----+---------+-----+--------------------+\n",
      "|42.0|       11066.8| 7.22|      8.0|    1|[42.0,11066.8,7.2...|\n",
      "|41.0|      11916.22|  6.5|     11.0|    1|[41.0,11916.22,6....|\n",
      "|38.0|      12884.75| 6.67|     12.0|    1|[38.0,12884.75,6....|\n",
      "|42.0|       8010.76| 6.71|     10.0|    1|[42.0,8010.76,6.7...|\n",
      "|37.0|       9191.58| 5.56|      9.0|    1|[37.0,9191.58,5.5...|\n",
      "+----+--------------+-----+---------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['Age', 'Total_Purchase', 'Years', 'Num_Sites'], outputCol='features')\n",
    "output = assembler.transform(data)\n",
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select(['features', 'Churn'])\n",
    "train_churn_data, test_churn_data = final_data.randomSplit([0.7,0.3])\n",
    "log_reg_churn = LogisticRegression(featuresCol='features', labelCol='Churn')\n",
    "\n",
    "fit_model = log_reg_churn.fit(train_churn_data)\n",
    "\n",
    "# results = fit_model.transform(test_churn_data)\n",
    "# results.select('Churn','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|Churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[25.0,9672.03,5.4...|  0.0|[4.26893268692309...|[0.98619648972642...|       0.0|\n",
      "|[26.0,8787.39,5.4...|  1.0|[0.65651176293813...|[0.65847636962466...|       0.0|\n",
      "|[26.0,8939.61,4.5...|  0.0|[5.90624225048043...|[0.99728499354820...|       0.0|\n",
      "|[27.0,8628.8,5.3,...|  0.0|[5.47474917849617...|[0.99582621382396...|       0.0|\n",
      "|[28.0,8670.98,3.9...|  0.0|[7.29661056265639...|[0.99932262678915...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_model.summary.predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              Churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|                619|                619|\n",
      "|   mean|0.15508885298869143|0.11147011308562198|\n",
      "| stddev|0.36228211868144927| 0.3149679240798625|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_model.summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|Churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[22.0,11254.38,4....|    0|[4.67528209792083...|[0.99076321874243...|       0.0|\n",
      "|[28.0,11128.95,5....|    0|[4.32620993417586...|[0.98695487649217...|       0.0|\n",
      "|[28.0,11204.23,3....|    0|[1.46172566526619...|[0.81179646890756...|       0.0|\n",
      "|[29.0,5900.78,5.5...|    0|[4.04733983517418...|[0.98283113642063...|       0.0|\n",
      "|[29.0,10203.18,5....|    0|[3.92395933308240...|[0.98062030245029...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_and_labels = fit_model.evaluate(test_churn_data)\n",
    "pred_and_labels.predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6909365312449014"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Churn')\n",
    "AUC = churn_eval.evaluate(pred_and_labels.predictions)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
